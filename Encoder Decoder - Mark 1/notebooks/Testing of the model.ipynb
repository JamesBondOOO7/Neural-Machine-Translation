{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adjusted-motor",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.007296,
     "end_time": "2021-06-23T12:25:31.337768",
     "exception": false,
     "start_time": "2021-06-23T12:25:31.330472",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Lets test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prepared-gibson",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T12:25:31.356872Z",
     "iopub.status.busy": "2021-06-23T12:25:31.356365Z",
     "iopub.status.idle": "2021-06-23T12:25:57.787821Z",
     "shell.execute_reply": "2021-06-23T12:25:57.786994Z",
     "shell.execute_reply.started": "2021-06-23T12:14:29.923581Z"
    },
    "papermill": {
     "duration": 26.443422,
     "end_time": "2021-06-23T12:25:57.787984",
     "exception": false,
     "start_time": "2021-06-23T12:25:31.344562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.3.1\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 14.7 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.5)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\r\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\r\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.2)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.1)\r\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.56.2)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\r\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.6.0.post20210108)\r\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.7.4.3)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.4.0)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.12.5)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.3)\r\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\r\n",
      "You can now load the model via spacy.load('en_core_web_sm')\r\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/en_core_web_sm -->\r\n",
      "/opt/conda/lib/python3.7/site-packages/spacy/data/en\r\n",
      "You can now load the model via spacy.load('en')\r\n",
      "Collecting de_core_news_sm==2.3.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.3.0/de_core_news_sm-2.3.0.tar.gz (14.9 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 14.9 MB 22.4 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from de_core_news_sm==2.3.0) (2.3.5)\r\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.5)\r\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (7.4.5)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.7.4)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (0.8.2)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.0.5)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (4.56.2)\r\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.0.0)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.19.5)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.25.1)\r\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.1.3)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.0.5)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (49.6.0.post20210108)\r\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.4.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.7.4.3)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2020.12.5)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (2.10)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_sm==2.3.0) (1.26.3)\r\n",
      "Building wheels for collected packages: de-core-news-sm\r\n",
      "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.3.0-py3-none-any.whl size=14907581 sha256=006e9a5445709d955cf85bb027f83f16f6094c2c9b6eed571a1fc8e683bd3de1\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-loenbkhs/wheels/75/30/c3/ea1c6002eede7f49c8ab017ce62a2981a87b1cd39fab6e6a65\r\n",
      "Successfully built de-core-news-sm\r\n",
      "Installing collected packages: de-core-news-sm\r\n",
      "Successfully installed de-core-news-sm-2.3.0\r\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\r\n",
      "You can now load the model via spacy.load('de_core_news_sm')\r\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/de_core_news_sm -->\r\n",
      "/opt/conda/lib/python3.7/site-packages/spacy/data/de\r\n",
      "You can now load the model via spacy.load('de')\r\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en\n",
    "!python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "together-embassy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T12:25:57.859035Z",
     "iopub.status.busy": "2021-06-23T12:25:57.858181Z",
     "iopub.status.idle": "2021-06-23T12:25:58.862383Z",
     "shell.execute_reply": "2021-06-23T12:25:58.861307Z",
     "shell.execute_reply.started": "2021-06-23T12:18:38.640300Z"
    },
    "papermill": {
     "duration": 1.047332,
     "end_time": "2021-06-23T12:25:58.862514",
     "exception": false,
     "start_time": "2021-06-23T12:25:57.815182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "# ---------------------------- ENCODER ----------------------------\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, drop_prob):\n",
    "        \"\"\"\n",
    "        :param input_size: the size of the input sequence\n",
    "        :param embedding_size: the embedding dimension\n",
    "        :param hidden_size: the hidden dimension used in the LSTM model\n",
    "        :param num_layers: number of layers in the LSTM model\n",
    "        :param drop_prob: the probability of dropout\n",
    "        \"\"\"\n",
    "\n",
    "        # self.param_dict = {\n",
    "        #     'input_size' : input_size,\n",
    "        #     'embedding_size' : embedding_size,\n",
    "        #     'hidden_size' : hidden_size,\n",
    "        #     'num_layers' : num_layers,\n",
    "        #     'drop_prob' : drop_prob\n",
    "        # }\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_prob)  # for Regularization\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        \n",
    "        # the rnn cell\n",
    "        self.rnn = nn.LSTM(input_size = embedding_size,\n",
    "                        hidden_size = hidden_size,\n",
    "                        num_layers = num_layers,\n",
    "                        dropout=drop_prob,\n",
    "                        batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: the vector form of the sentence \n",
    "                  (containing the indicies mapped in the vocab)\n",
    "        \"\"\"\n",
    "\n",
    "        # pass the data\n",
    "        # N X T --> N X T X D\n",
    "        x = self.dropout(self.embedding(x))\n",
    "\n",
    "        output, (hidden_state, cell_state) = self.rnn(x)\n",
    "\n",
    "        # return the context vectors\n",
    "        # their shape : L X N X H (num_layers X batch_size X hidden_size)\n",
    "        return hidden_state, cell_state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------- DECODER ----------------------------\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, drop_prob, output_size):\n",
    "        \"\"\"\n",
    "        :param input_size: the size of the input sequence\n",
    "        :param embedding_size: the embedding dimension\n",
    "        :param hidden_size: the hidden dimension used in the LSTM model\n",
    "        :param num_layers: number of layers in the LSTM model\n",
    "        :param drop_prob: the probability of dropout\n",
    "        :param output_size: the output size of the linear layer after the decoding\n",
    "        \"\"\"\n",
    "\n",
    "        # self.param_dict = {\n",
    "        #     'input_size' : input_size,\n",
    "        #     'embedding_size' : embedding_size,\n",
    "        #     'hidden_size' : hidden_size,\n",
    "        #     'num_layers' : num_layers,\n",
    "        #     'drop_prob' : drop_prob,\n",
    "        #     'output_size' : output_size\n",
    "        # }\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(drop_prob)  # for Regularization\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=embedding_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            dropout=drop_prob,\n",
    "                            # batch_first=True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "\n",
    "        # unsqueeze x\n",
    "        # shape becomes : 1 X N\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        # 1 X N --> 1 X N X D\n",
    "        x = self.dropout(self.embedding(x))\n",
    "\n",
    "        # shape of outputs : 1 X N X H (1 X batch_size X Hidden_size)\n",
    "        # shape of hidden and cell states : L X N X H\n",
    "        outputs, (hidden_state, cell_state) = self.rnn(x, (hidden_state, cell_state))\n",
    "\n",
    "        # 1 X N X H --> 1 X N X output_size\n",
    "        predictions = self.fc(outputs)\n",
    "\n",
    "        # 1 X N X output_size --> N X output_size\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        return predictions, hidden_state, cell_state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------- SEQUENCE-TO-SEQUENCE ----------------------------\n",
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self, Encoder_LSTM, Decoder_LSTM):\n",
    "        \"\"\"\n",
    "        :param Encoder_LSTM: the encoder part for the Seq2Seq model\n",
    "        :param Decoder_LSTM: the decoder part for the Seq2Seq model\n",
    "        \"\"\"\n",
    "\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.Encoder_LSTM = Encoder_LSTM\n",
    "        self.Decoder_LSTM = Decoder_LSTM\n",
    "\n",
    "    def forward(self, source, target, eng_vocab_size, tfr=0.5):\n",
    "        \"\"\"\n",
    "        :param source: padded sentences in German\n",
    "                       shape : [(sentence length German + some padding), #Sentences]\n",
    "        :param target: padded sentences in English\n",
    "                       shape : [(sentence length English + some padding), #Sentences]\n",
    "        :param eng_vocab_size : size of the english vocab\n",
    "        :param tfr: teach force ratio\n",
    "        \"\"\"\n",
    "\n",
    "        # # Convert it into Batch Size X Sequence Length\n",
    "        # target = target.permute(1, 0)\n",
    "\n",
    "        batch_size = source.shape[0]\n",
    "        target_len = target.shape[0]\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, eng_vocab_size).to(device)\n",
    "\n",
    "        # retaining the context vector from the encoder\n",
    "        hidden_state, cell_state = self.Encoder_LSTM(source)\n",
    "\n",
    "        x = target[0]\n",
    "\n",
    "        for i in range(1, target_len):\n",
    "\n",
    "            # output : batch_size X |Eng_Vocab_Size|\n",
    "            output, hidden_state, cell_state = self.Decoder_LSTM(x, hidden_state, cell_state)\n",
    "\n",
    "            outputs[i] = output\n",
    "\n",
    "            best_guess = output.argmax(1)  # the most suitable word embedding\n",
    "\n",
    "            # Teach force ratio\n",
    "            # Either pass the next correct word from the dataset\n",
    "            # or use the predicted word\n",
    "            x = target[i] if random.random() < tfr else best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sized-jimmy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T12:25:58.923159Z",
     "iopub.status.busy": "2021-06-23T12:25:58.922315Z",
     "iopub.status.idle": "2021-06-23T12:26:00.193289Z",
     "shell.execute_reply": "2021-06-23T12:26:00.192632Z",
     "shell.execute_reply.started": "2021-06-23T12:20:26.608628Z"
    },
    "papermill": {
     "duration": 1.306055,
     "end_time": "2021-06-23T12:26:00.193463",
     "exception": false,
     "start_time": "2021-06-23T12:25:58.887408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic utilities needed in the code\n",
    "\n",
    "import torch\n",
    "import spacy\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
    "    \"\"\"\n",
    "    This function translates the input german sentence to the english sentence.\n",
    "    German sentence --> German Vector --> Encoder --> context vector --> Decoder --> English Vector --> English Sentence\n",
    "\n",
    "    :param model: the sequence-to-sequnce model\n",
    "    :param sentence: the input \"german\" sentence\n",
    "    :param german: the german Field object\n",
    "    :param english : the english Field object\n",
    "    :param device: cuda / cpu\n",
    "    :param max_length : maximum length of the translated sentence\n",
    "    \"\"\"\n",
    "\n",
    "    spacy_german = spacy.load(\"de\")\n",
    "\n",
    "    if type(sentence) == str:\n",
    "        tokens = [token.text.lower() for token in spacy_german(sentence)]\n",
    "\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    # insert the start and end sequence\n",
    "    tokens.insert(0, german.init_token)\n",
    "    tokens.append(german.eos_token)\n",
    "\n",
    "    text_to_indicies = [german.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    # (N, ) --> (1 X N)\n",
    "    sentence_tensor = torch.LongTensor(text_to_indicies).unsqueeze(0).to(device)\n",
    "\n",
    "    # Retrieve the hidden_state and cell_state from the encoder\n",
    "    with torch.no_grad():\n",
    "        hidden_state, cell_state = model.Encoder_LSTM(sentence_tensor)\n",
    "\n",
    "    # start the decoding part using start sequence and the (hidden_state, cell_state)\n",
    "    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden_state, cell_state = model.Decoder_LSTM(previous_word, hidden_state, cell_state)\n",
    "\n",
    "            # shape received : 1 X 1 X |Eng_Vocab|; squeeze it\n",
    "            # output = output.squeeze(0)\n",
    "\n",
    "            best_guess = output.argmax(1).item()\n",
    "\n",
    "        outputs.append(best_guess)\n",
    "\n",
    "        # Model stops predicting if it predicts <eos> token (index)\n",
    "        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    # We have the indicies of the translated sentence in english\n",
    "    # Now, we will predict the sentence\n",
    "    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
    "\n",
    "    return translated_sentence[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "burning-friend",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T12:26:00.259041Z",
     "iopub.status.busy": "2021-06-23T12:26:00.258422Z",
     "iopub.status.idle": "2021-06-23T12:26:20.532809Z",
     "shell.execute_reply": "2021-06-23T12:26:20.533221Z",
     "shell.execute_reply.started": "2021-06-23T12:20:30.274447Z"
    },
    "papermill": {
     "duration": 20.314362,
     "end_time": "2021-06-23T12:26:20.533386",
     "exception": false,
     "start_time": "2021-06-23T12:26:00.219024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'man', 'in', 'a', 'blue', 'shirt', 'is', 'standing', 'on', 'a', 'ladder', 'cleaning', 'a', 'window', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "import spacy\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "\n",
    "def tokenize_german(text):\n",
    "    \"\"\"\n",
    "    tokenizer for German language\n",
    "    \"\"\"\n",
    "    return [token.text for token in spacy_german.tokenizer(text)]\n",
    "\n",
    "def tokenize_english(text):\n",
    "    \"\"\"\n",
    "    tokenizer for English language\n",
    "    \"\"\"\n",
    "    return [token.text for token in spacy_english.tokenizer(text)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    \n",
    "    # tokenizers for German and English\n",
    "    spacy_german = spacy.load(\"de\")\n",
    "    spacy_english = spacy.load(\"en\")\n",
    "\n",
    "    # Lets build the vocab\n",
    "    german = Field(tokenize=tokenize_german,\n",
    "                    lower=True,\n",
    "                    init_token=\"<sos>\",\n",
    "                    eos_token=\"<eos>\"\n",
    "    )\n",
    "\n",
    "    # Field Object for English\n",
    "    english = Field(tokenize=tokenize_english,\n",
    "                    lower=True,\n",
    "                    init_token=\"<sos>\",\n",
    "                    eos_token=\"<eos>\"\n",
    "    )\n",
    "\n",
    "    # dataset object\n",
    "    dataset = TabularDataset(path=\"../input/german-to-english/dataset.csv\",\n",
    "                            format='csv',\n",
    "                            skip_header=True,\n",
    "                            fields=[('ger_sent', german), ('eng_sent', english)]\n",
    "    )\n",
    "\n",
    "    # BUILDING THE VOCAB\n",
    "    german.build_vocab(dataset, max_size=10000, min_freq=3)\n",
    "    english.build_vocab(dataset, max_size=10000, min_freq=3)\n",
    "\n",
    "    GERMAN_VOCAB = german.vocab\n",
    "    ENGLISH_VOCAB = english.vocab\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # load the model\n",
    "    checkpoint = torch.load(\"../input/full-training-data-encoder-decoder-nmt/checkpoint-NMT-BEST.pth\")\n",
    "    my_model = checkpoint['model']\n",
    "\n",
    "\n",
    "    # load the state dict of the model\n",
    "    sd = torch.load(\"../input/full-training-data-encoder-decoder-nmt/checkpoint-NMT-BEST-SD.pth\")\n",
    "    my_model.load_state_dict(sd)\n",
    "\n",
    "    sentence = \"ein mann in einem blauen hemd steht auf einer leiter und putzt ein fenster\"\n",
    "    my_model.to(device)\n",
    "\n",
    "    # Let's translate some sentences\n",
    "    print(translate_sentence(my_model, sentence, german, english, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "concrete-reynolds",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T12:26:20.584125Z",
     "iopub.status.busy": "2021-06-23T12:26:20.583591Z",
     "iopub.status.idle": "2021-06-23T12:26:21.887150Z",
     "shell.execute_reply": "2021-06-23T12:26:21.886204Z",
     "shell.execute_reply.started": "2021-06-23T12:22:40.429228Z"
    },
    "papermill": {
     "duration": 1.330227,
     "end_time": "2021-06-23T12:26:21.887290",
     "exception": false,
     "start_time": "2021-06-23T12:26:20.557063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'little', 'girl', 'is', 'sitting', 'in', 'front', 'of', 'a', 'large', 'painted', 'rainbow', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Ein kleines Mädchen sitzt vor einem großen gemalten Regenbogen.\"\n",
    "print(translate_sentence(my_model, sentence, german, english, device))\n",
    "\n",
    "# GOOGLE : A little girl is sitting in front of a large painted rainbow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "driven-hungarian",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-23T12:26:21.941537Z",
     "iopub.status.busy": "2021-06-23T12:26:21.940740Z",
     "iopub.status.idle": "2021-06-23T12:26:23.255049Z",
     "shell.execute_reply": "2021-06-23T12:26:23.255476Z",
     "shell.execute_reply.started": "2021-06-23T12:23:09.707884Z"
    },
    "papermill": {
     "duration": 1.344381,
     "end_time": "2021-06-23T12:26:23.255633",
     "exception": false,
     "start_time": "2021-06-23T12:26:21.911252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'large', 'crowd', 'of', 'people', 'stand', 'outside', 'of', 'the', '<unk>', 'while', 'wearing', 'front', 'of', 'a', 'metro', 'station', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Eine große Menschenmenge steht außen vor dem Eingang einer Metrostation.\"\n",
    "print(translate_sentence(my_model, sentence, german, english, device))\n",
    "\n",
    "# GOOGLE : A large crowd stands outside the entrance of a metro station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-hydrogen",
   "metadata": {
    "papermill": {
     "duration": 0.023627,
     "end_time": "2021-06-23T12:26:23.303453",
     "exception": false,
     "start_time": "2021-06-23T12:26:23.279826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 58.124907,
   "end_time": "2021-06-23T12:26:24.436989",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-23T12:25:26.312082",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
